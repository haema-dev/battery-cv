import argparse
import os
from pathlib import Path
import torch
from anomalib.data import Folder
from anomalib.models import Fastflow
from anomalib.engine import Engine

# ì‚¬ìš©ìë‹˜ì˜ ìš”ì²­ì— ë”°ë¥¸ 'ì¼ê´€ì„±' í™•ë³´: ì¶”ë¡  ì „ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ TorchInferencerë„ í•¨ê»˜ ì¤€ë¹„
try:
    from anomalib.deploy import TorchInferencer
    HAS_INFERENCER = True
except ImportError:
    HAS_INFERENCER = False

def run_pipeline(data_path, output_dir, epochs):
    print("--------------------------------------------------")
    print(f"ğŸš€ [Stage 1] FastFlow Training Pipeline (v2: 100e)")
    print(f"ğŸ“ Data: {data_path}")
    print(f"â²ï¸ Target Epochs: {epochs}")
    print(f"ğŸ› ï¸ Inferencer Ready: {HAS_INFERENCER}")
    print("--------------------------------------------------")

    # 1. ë°ì´í„° ëª¨ë“ˆ ì„¤ì • (Anomalib 1.x ìµœì‹  API ëŒ€ì‘)
    # ë¡œê·¸ í™•ì¸ ê²°ê³¼ 'test_dir' ì¸ìê°€ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ 'normal_test_dir'ë¡œ ìˆ˜ì •
    datamodule = Folder(
        name="battery",
        root=data_path,
        normal_dir="train/good",
        normal_test_dir="test/good",    # test_dir ëŒ€ì‹  êµ¬ì²´ì ì¸ ê²½ë¡œ ì§€ì •
        test_split_mode="from_dir",
        task="classification",
        image_size=(256, 256)
    )

    # 2. ëª¨ë¸ ì„¤ì • (FastFlow)
    model = Fastflow(backbone="resnet18", flow_steps=8)

    # 3. ì—”ì§„ ì„¤ì • (T4 GPU ì‚¬ìš©)
    engine = Engine(
        max_epochs=epochs,
        default_root_dir=output_dir,
        devices=1,
        accelerator="auto",
        task="classification"
    )

    # 4. í•™ìŠµ ì‹œì‘
    print("â³ Starting training...")
    engine.fit(model=model, datamodule=datamodule)
    
    # 5. ê²°ê³¼ë¬¼ ì €ì¥
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ì¶”ë¡  ë‹¨ê³„(Stage 2)ì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ ì €ì¥
    model_save_path = output_path / "model.pt"
    torch.save(model.state_dict(), model_save_path)
    
    print(f"âœ… Training completed. Weights saved: {model_save_path}")

    # 6. [ì¼ê´€ì„± ê²€ì¦] TorchInferencerë¡œ ë¡œë“œ ê°€ëŠ¥í•œì§€ í™•ì¸
    if HAS_INFERENCER:
        try:
            print("ğŸ” Verifying model consistency with TorchInferencer...")
            # ê²€ì¦ ì‹œì—ëŠ” cpuë¡œ ë¡œë“œ í…ŒìŠ¤íŠ¸
            inferencer = TorchInferencer(path=model_save_path, device="cpu")
            print("âœ¨ Success: Model is compatible with TorchInferencer API.")
        except Exception as e:
            print(f"âš ï¸ Note: Inferencer verification skipped or errored: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="Path to dataset")
    parser.add_argument("--output_dir", type=str, required=True, help="Path to save outputs")
    parser.add_argument("--epochs", type=int, default=100, help="Number of epochs")
    
    args = parser.parse_args()
    run_pipeline(args.data_path, args.output_dir, args.epochs)