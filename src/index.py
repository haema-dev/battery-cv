import os, torch, argparse, mlflow, json, time
from loguru import logger
from anomalib.models import Fastflow
from anomalib.data import Folder
from anomalib.engine import Engine
import numpy as np, cv2

# í´ë¼ìš°ë“œ í™˜ê²½ íŒ¨í‚¤ì§€ ëˆ„ë½ ëŒ€ì‘ (adlfs, fsspec)
try:
    import adlfs
    import fsspec
except ImportError:
    import subprocess, sys
    logger.warning("ğŸ“¦ í•„ìˆ˜ íŒ¨í‚¤ì§€(adlfs, fsspec)ê°€ ëˆ„ë½ë˜ì–´ ëŸ°íƒ€ì„ ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "adlfs", "fsspec"])
    import adlfs
    import fsspec

# ë…ë¦½ ëª¨ë“ˆ ì„í¬íŠ¸
from extractor import run_selective_extraction

def main():

    # ================== 1. input/output ì„¤ì • ==================== #
    parser = argparse.ArgumentParser()
    parser.add_argument('--output_dir', type=str, default='./outputs')
    
    # ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ SAS ê¸°ë°˜ ì¸ì
    parser.add_argument("--account_name", type=str, default="batterydata8ai6team")
    parser.add_argument("--sas_token", type=str, help="Azure Storage SAS Token")
    parser.add_argument("--container", type=str, default="battery-data-zip")
    parser.add_argument("--blob_path", type=str, default="TS_Exterior_Img_Datasets_images_3.zip")
    parser.add_argument("--good_list_path", type=str, default="good_list.csv")
    parser.add_argument("--epochs", type=int, default=10)
    
    args = parser.parse_args()
    
    mlflow.start_run()
    OUTPUT_DIR = args.output_dir
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logger.info(f"ğŸ“‚ {os.path.abspath(OUTPUT_DIR)}")
    
    try:
        # ================== 2. ì´ìƒíƒì§€ ì‘ì—… ==================== #
        
        # [A] ë°ì´í„° ìë™ ì¶”ì¶œ (extractor ëª¨ë“ˆ ì‚¬ìš©)
        dataset_root = "./temp_datasets"
        normal_dir = os.path.join(dataset_root, "normal")
        
        # ì§ì ‘ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì¶œ ìˆ˜í–‰ (ë§ˆìš´íŠ¸ ì—†ì´ SAS í† í° ì‚¬ìš©)
        success = run_selective_extraction(
            account_name=args.account_name,
            sas_token=args.sas_token,
            container=args.container,
            blob_path=args.blob_path,
            good_list_path=args.good_list_path,
            output_dir=normal_dir
        )

        if not success:
            raise RuntimeError("í•™ìŠµ ë°ì´í„° ì¤€ë¹„(ì¶”ì¶œ) ì‹¤íŒ¨")

        # ====== Anomalib FastFlow í•™ìŠµ ====== 
        logger.info("ğŸš€ Fastflow í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        datamodule = Folder(
            name="battery_anomaly",
            root=dataset_root,
            normal_dir="normal",
            train_batch_size=4,
            num_workers=4,
        )

        model = Fastflow(backbone="resnet18", flow_steps=8)

        engine = Engine(
            max_epochs=args.epochs,
            accelerator="gpu",
            devices=1,
            limit_val_batches=0,
            num_sanity_val_steps=0,
            default_root_dir=OUTPUT_DIR
        )

        engine.fit(datamodule=datamodule, model=model)

        # ê²°ê³¼ ë³€ìˆ˜ ì„¤ì • (ê¸°ì¡´ í…œí”Œë¦¿ í˜¸í™˜ìš©)
        score = 0.0 # í•™ìŠµìš©ì´ë¯€ë¡œ ë”ë¯¸ê°’
        label = "N/A"
        result = np.zeros((100, 100, 3), dtype=np.uint8) # ë”ë¯¸ ì´ë¯¸ì§€
        # ====== ì—¬ê¸°ê¹Œì§€ =======

        # mlflow ì— ì¶”ê°€í•  ê²°ê³¼ë“¤ì´ ìˆìœ¼ë©´ ì¶”ê°€í•´ë„ ë¨. ì—†ìœ¼ë©´ ì‚­ì œ.
        cv2.imwrite(f"{OUTPUT_DIR}/result.jpg", result)
        model_path = f"{OUTPUT_DIR}/model.pt"
        torch.save(model.state_dict(), model_path)
        with open(f"{OUTPUT_DIR}/info.json", 'w') as f:
            json.dump({
                "model": "FastFlow",
                "backbone": "resnet18",
                "zip_source": args.blob_path,
                "finish_time": time.ctime()
            }, f)


        # ================== 3. output blob mount ==================== #
        logger.success(f"âœ… {score:.3f} ({label})")
        mlflow.log_artifact(OUTPUT_DIR)
                
    except Exception as e:
        logger.error(f"âŒ {e}")
        raise

    mlflow.end_run()
    logger.success("ğŸ‰ ì™„ë£Œ!")

if __name__ == "__main__": main()
