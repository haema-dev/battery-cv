import argparse
import os
import sys
import json
import time
from pathlib import Path
import torch
import cv2
import numpy as np
from loguru import logger
from collections import defaultdict

# Anomalib TorchInferencer ì‚¬ìš©
try:
    from anomalib.deploy import TorchInferencer
    INFERENCER_AVAILABLE = True
except ImportError:
    INFERENCER_AVAILABLE = False

def find_validation_root(base_path):
    """ì‚¬ìš©ìë‹˜ì´ ì§€ì •í•˜ì‹  'datasets/256x256 fit/validation' ê²½ë¡œë¥¼ ì •ë°€ íƒìƒ‰í•©ë‹ˆë‹¤."""
    base = Path(base_path).resolve()
    logger.info(f"[*] ê²€ì¦ ë°ì´í„° íƒìƒ‰ ì‹œì‘: {base}")
    
    # 1ìˆœìœ„: 'datasets/256x256 fit/validation' ì •ë°€ íƒìƒ‰
    for p in base.rglob("*/validation"):
        if "256x256 fit" in str(p):
            logger.success(f"OK: ê²€ì¦ ë°ì´í„°ì…‹ ë°œê²¬: {p}")
            return p
            
    # 2ìˆœìœ„: 'validation' í´ë” íƒìƒ‰
    for p in base.rglob("validation"):
        if p.is_dir():
            logger.warning(f"WARN: 'validation' í´ë” ë°œê²¬: {p}")
            return p
            
    logger.error("ERR: 'validation' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return None

def run_evaluation(data_path, model_path, output_dir):
    logger.info("==================================================")
    logger.info("STAGE 2: INFERENCE & PERFORMANCE EVALUATION")
    logger.info("==================================================")

    if not INFERENCER_AVAILABLE:
        logger.error("âŒ 'TorchInferencer'ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. ëª¨ë¸ ìˆ˜ë™ ì¡°ë¦½ (Architecture Reconstruction)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    try:
        from anomalib.models import Fastflow
        
        # [RECONSTRUCTION] ì„¤ê³„ë„(ë¼ˆëŒ€) ë¨¼ì € ì„¸ìš°ê¸°: resnet18 ê¸°ë°˜ì˜ Fastflow
        logger.info("ğŸ—ï¸ ëª¨ë¸ ì„¤ê³„ë„(Fastflow-ResNet18) ê¸°ë°˜ ë¼ˆëŒ€ ìƒì„± ì¤‘...")
        model = Fastflow(backbone="resnet18")
        
        # ê°€ì¤‘ì¹˜ íŒŒì¼ ë¡œë“œ
        if not os.path.exists(model_path):
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            return
            
        ckpt = torch.load(model_path, map_location="cpu")
        
        # ê°€ì¤‘ì¹˜ ì •ì œ (state_dict ì¶”ì¶œ)
        # ckptê°€ lightning í˜•ì‹({"state_dict": ...})ì´ê±°ë‚˜ raw state_dictì¼ ê²½ìš° ëŒ€ì‘
        state_dict = ckpt.get("state_dict", ckpt) if isinstance(ckpt, dict) else ckpt
        
        # ê°„í˜¹ 'model' í‚¤ë¡œ í•œ ë²ˆ ë” ê°ì‹¸ì ¸ ìˆëŠ” ê²½ìš° ëŒ€ì‘
        if isinstance(state_dict, dict) and "model" in state_dict:
            state_dict = state_dict["model"]
            
        # ë¼ˆëŒ€ì— ì§€ëŠ¥(ê°€ì¤‘ì¹˜) ì£¼ì…
        model.load_state_dict(state_dict, strict=False)
        model.to(device)
        model.eval() # ëª…ì‹œì ìœ¼ë¡œ eval ëª¨ë“œ ì „í™˜
        logger.success("âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì •ë°€ ì¡°ë¦½ ì™„ë£Œ!")

        # ì¡°ë¦½ëœ 'ê°ì²´(nn.Module)'ë¥¼ TorchInferencerê°€ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì„ì‹œ ì €ì¥
        # TorchInferencerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ torch.load(path)['model']ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì•„ì˜ˆ ê°ì²´ë¥¼ ê¸°ëŒ€í•¨
        temp_model_path = "/tmp/reconstructed_model.pt"
        os.makedirs("/tmp", exist_ok=True)
        torch.save({"model": model}, temp_model_path)
        
        # ìµœì¢…ì ìœ¼ë¡œ ì¡°ë¦½ëœ ëª¨ë¸ì˜ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
        logger.info(f"ğŸ’¾ ì¡°ë¦½ëœ ëª¨ë¸ ì„ì‹œ ì €ì¥: {temp_model_path}")
        inferencer = TorchInferencer(path=temp_model_path, device=device)
        logger.success("âœ… ìµœì¢… TorchInferencer ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ì¡°ë¦½ ë° ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return

    # 2. ê²½ë¡œ ì„¤ì •
    validation_root = find_validation_root(data_path)
    if not validation_root: return
    
    output_base = Path(output_dir)
    output_base.mkdir(parents=True, exist_ok=True)

    # 3. í‰ê°€ ë°ì´í„° ì´ˆê¸°í™” (Confusion Matrixìš©)
    results_summary = []
    matrix = defaultdict(int) 

    # 4. ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
    categories = [d for d in validation_root.iterdir() if d.is_dir()]
    logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ëª©ë¡: {[c.name for c in categories]}")

    for cat_dir in categories:
        cat_name = cat_dir.name
        is_actual_anomaly = 0 if cat_name.lower() == "good" else 1
        
        cat_output = output_base / "heatmaps" / cat_name
        cat_output.mkdir(parents=True, exist_ok=True)
        
        img_files = list(cat_dir.glob("*.jpg")) + list(cat_dir.glob("*.png")) + list(cat_dir.glob("*.jpeg"))
        logger.info(f"ğŸ” {cat_name} ì²˜ë¦¬ ì¤‘... ({len(img_files)}ì¥)")

        for img_path in img_files:
            try:
                # ì¶”ë¡  ìˆ˜í–‰
                prediction = inferencer.predict(image=str(img_path))
                
                # ì‹œê°í™” ì €ì¥ (Heatmap)
                heatmap = prediction.heatmap
                cv2.imwrite(str(cat_output / f"heatmap_{img_path.name}"), heatmap)
                
                # ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ
                pred_label = int(prediction.pred_label) if hasattr(prediction, 'pred_label') else (1 if prediction.pred_score > 0.5 else 0)
                pred_score = float(prediction.pred_score)

                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if is_actual_anomaly == 0: 
                    if pred_label == 0: matrix["TN"] += 1
                    else: matrix["FP"] += 1
                else: 
                    if pred_label == 1: matrix["TP"] += 1
                    else: matrix["FN"] += 1
                
                results_summary.append({
                    "image": img_path.name,
                    "actual": "Anomaly" if is_actual_anomaly else "Normal",
                    "predicted": "Anomaly" if pred_label else "Normal",
                    "score": pred_score
                })

            except Exception as e:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨ ({img_path.name}): {e}")

    # 5. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
    total = sum(matrix.values())
    accuracy = (matrix["TP"] + matrix["TN"]) / total if total > 0 else 0
    
    logger.info("--------------------------------------------------")
    logger.info("ğŸ“Š STAGE 2 EVALUATION REPORT")
    logger.info(f"âœ… Accuracy: {accuracy:.4f}")
    logger.info(f"ğŸ“ Confusion Matrix: {dict(matrix)}")
    logger.info("--------------------------------------------------")

    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    report = {
        "metrics": dict(matrix),
        "overall_accuracy": accuracy,
        "details": results_summary
    }
    with open(output_base / "evaluation_report.json", "w") as f:
        json.dump(report, f, indent=4)
    
    logger.success(f"ğŸ‰ Stage 2 ì™„ë£Œ. íˆíŠ¸ë§µ ë° ë¦¬í¬íŠ¸ ì €ì¥ë¨: {output_dir}")

if __name__ == "__main__":
    # ë””ë²„ê¹…: ì—ì €ì—ì„œ ë“¤ì–´ì˜¤ëŠ” ì›ë³¸ ì¸ì í™•ì¸
    logger.info(f"ğŸ“‹ Raw Arguments: {sys.argv}")
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="Path to input validation folders")
    parser.add_argument("--model_path", type=str, required=True, help="Path to trained model.pt")
    parser.add_argument("--output_dir", type=str, required=True, help="Folder to save results")
    
    try:
        args = parser.parse_args()
        logger.info(f"âœ… Parsed Arguments: data={args.data_path}, model={args.model_path}, out={args.output_dir}")
        
        sys.stdout.reconfigure(line_buffering=True)
        run_evaluation(args.data_path, args.model_path, args.output_dir)
    except Exception as e:
        logger.error(f"âŒ FATAL: Argument issue: {e}")
        sys.exit(1)