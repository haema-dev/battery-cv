# -*- coding: utf-8 -*-
# Version trigger for Azure ML - v6 (Strict Compliance)
import os
import sys
import torch
import argparse
import mlflow
import json
import time
import cv2
import random
import numpy as np
from loguru import logger
from anomalib.models import Fastflow
from torch import optim
from anomalib.data import Folder
from anomalib.engine import Engine
from anomalib.loggers import AnomalibMLFlowLogger
from pathlib import Path
from torchvision.transforms.v2 import Compose, Normalize, Resize
from lightning.pytorch.callbacks import EarlyStopping
import lightning

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def convert_to_lightning_checkpoint(model_path, model, output_dir, transform=None):
    """
    [Rigorous Final Fix] raw state_dictë¥¼ Anomalib/Lightning ì •ì‹ ì²´í¬í¬ì¸íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - BestFit Matcher: ëª¨ë¸ì˜ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° 'model.' ì ‘ë‘ì–´ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê°€ê³µí•©ë‹ˆë‹¤.
    - Strict Filtering: ëª¨ë¸ì— ì—†ëŠ” í‚¤(ì˜ˆ: êµ¬ë²„ì „ post_processor ë“±)ë¥¼ ì œê±°í•˜ì—¬ ì—”ì§„ì˜ strict ë¡œë“œë¥¼ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
    """
    logger.info(f"[*] ê°€ì¤‘ì¹˜ ê·œê²© ë³€í™˜ ë° ë˜í•‘ ì‹œì‘: {model_path}")
    raw_ckpt = torch.load(model_path, map_location="cpu")
    
    # 1. ì—¬ëŸ¬ í¬ë§·(Anomalib/Lightning/Raw)ì—ì„œ state_dict ì¶”ì¶œ
    if isinstance(raw_ckpt, dict):
        state_dict = raw_ckpt.get("state_dict", raw_ckpt.get("model", raw_ckpt))
    else:
        state_dict = raw_ckpt

    # 2. BestFit Matcher: ëª¨ë¸ì˜ ì‹¤ì œ ìš”êµ¬ì‚¬í•­ê³¼ ì²´í¬í¬ì¸íŠ¸ í‚¤ ëŒ€ì¡°
    model_state = model.state_dict()
    model_keys = set(model_state.keys())
    
    # [Robustness] LightningModuleì´ ì•„ì§ setupë˜ì§€ ì•Šì•„ state_dictê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ë‚´ë¶€ ëª¨ë¸ í™•ì¸
    if not model_keys and hasattr(model, "model"):
        logger.info("[*] LightningModule í‚¤ê°€ ë¹„ì–´ìˆìŒ. ë‚´ë¶€ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")
        inner_keys = set(model.model.state_dict().keys())
        # Anomalib LightningModuleì€ ë³´í†µ ë‚´ë¶€ ëª¨ë¸ í‚¤ì— 'model.'ì„ ë¶™ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.
        model_keys = {f"model.{k}" for k in inner_keys}

    strategies = [
        ("As-is", lambda d: d),
        ("Add 'model.'", lambda d: {f"model.{k}": v for k, v in d.items()}),
        ("Remove 'model.'", lambda d: {k[6:] if k.startswith("model.") else k: v for k, v in d.items()})
    ]
    
    best_matching_dict = state_dict
    max_matches = 0
    best_strategy = "None"
    
    num_model_keys = len(model_keys)
    logger.info(f"[*] ë§¤ì¹­ ì „ëµ íƒìƒ‰ ì‹œì‘ (ëª¨ë¸ í‚¤ ì´ {num_model_keys}ê°œ)")
    
    if num_model_keys > 0:
        for name, func in strategies:
            try:
                test_dict = func(state_dict)
                matches = len(model_keys.intersection(test_dict.keys()))
                logger.info(f"    - ì „ëµ '{name}': {matches}ê°œ ë§¤ì¹­")
                if matches > max_matches:
                    max_matches = matches
                    best_strategy = name
                    best_matching_dict = test_dict
            except Exception:
                continue
        
        match_rate = (max_matches / num_model_keys) * 100
        logger.info(f"[*] ìµœì¢… ì±„íƒ ì „ëµ: {best_strategy} (ë§¤ì¹­ë¥ : {match_rate:.1f}%)")
    else:
        logger.warning("[!] ëª¨ë¸ í‚¤ë¥¼ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì „ëµ(As-is)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        best_strategy = "Default (As-is)"

    # 3. Strict Filtering: ëª¨ë¸ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¶ˆí•„ìš”í•œ í‚¤ ì œê±° (RuntimeError ë°©ì§€)
    final_state_dict = {k: v for k, v in best_matching_dict.items() if k in model_keys}
    
    # 4. ì •ì‹ ê·œê²© ë˜í•‘
    lightning_ckpt = {
        "state_dict": final_state_dict,
        "epoch": 0,
        "global_step": 0,
        "pytorch-lightning_version": getattr(lightning, "__version__", "2.1.0"),
        "transform": transform,
        "callbacks": {},
        "optimizer_states": [],
        "lr_schedulers": []
    }
    
    wrapped_path = Path(output_dir) / "wrapped_checkpoint.ckpt"
    torch.save(lightning_ckpt, wrapped_path)
    return str(wrapped_path)

def main():
    # ================== 1. Input/Output ì„¤ì • ==================== #
    parser = argparse.ArgumentParser()    
    parser.add_argument("--data_path", type=str, required=True, help="Path to mounted data asset")
    parser.add_argument("--model_path", type=str, default=None, help="Path to pre-trained model checkpoint")
    parser.add_argument('--output_dir', type=str, default='./outputs')
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--backbone", type=str, default="resnet18", help="Feature extractor backbone")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument("--lr", type=float, default=0.0001, help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=1e-5, help="Weight decay")
    parser.add_argument("--mode", type=str, default="evaluation", choices=["training", "evaluation"])

    args = parser.parse_args()
    set_seed(args.seed)
    
    OUTPUT_DIR = Path(args.output_dir)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    logger.info("==================================================")
    logger.info(" STAGE 2: PM Selection - FastFlow Training/Eval")
    logger.info(f" ë°ì´í„° ê²½ë¡œ: {args.data_path}")
    if args.model_path:
        logger.info(f" ëª¨ë¸ ë¡œë“œ ê²½ë¡œ: {args.model_path}")
    logger.info(f" ì„¤ì •: Backbone={args.backbone}, Mode={args.mode}")
    logger.info("==================================================")

    try:
        # ================== 2. Anomalib ë°ì´í„° êµ¬ì„± ==================== #
        dataset_root = Path(args.data_path)
        val_path = dataset_root / "validation"
        
        # ë¶ˆëŸ‰ ì¹´í…Œê³ ë¦¬ ìë™ ê°ì§€
        abnormal_dirs = []
        if val_path.exists():
            abnormal_dirs = [f"validation/{d.name}" for d in val_path.iterdir() if d.is_dir() and d.name != "good"]
        logger.info(f"[*] ë¶ˆëŸ‰ ì¹´í…Œê³ ë¦¬ ê°ì§€: {abnormal_dirs}")

        transform = Compose([
            Resize((256, 256)),
            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        datamodule = Folder(
            name="battery",
            root=str(dataset_root),
            normal_dir="train/good",
            normal_test_dir="validation/good",
            abnormal_dir=abnormal_dirs if abnormal_dirs else None,
            train_batch_size=32,
            eval_batch_size=8,
            num_workers=4,
            train_transform=transform,
            eval_transform=transform,
            task="classification",
            seed=args.seed
        )

        # ================== 3. ëª¨ë¸ ìƒì„± ë° ìˆ˜ë™ ì´ˆê¸°í™” ==================== #
        logger.info(f"ğŸ—ï¸ ëª¨ë¸ ìƒì„± ì¤‘: FastFlow ({args.backbone})")
        model = Fastflow(backbone=args.backbone, flow_steps=8)
        
        # [Strict Fix] ì—”ì§„ êµ¬ë™ ì „ ëª¨ë¸ ë ˆì´ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìƒì„± (Key ê°ì§€ìš©)
        # Anomalib 1.1.3ì—ì„œëŠ” setup()ì„ í˜¸ì¶œí•´ì•¼ ë‚´ë¶€ ë ˆì´ì–´(feature_extractor ë“±)ê°€ ì‹¤ì²´í™”ë©ë‹ˆë‹¤.
        datamodule.setup(stage="test")
        model.setup()

        # ================== 4. ê°€ì¤‘ì¹˜ ë˜í•‘ (Rigorous Fix) ==================== #
        # ì—”ì§„ì´ "ì§ì ‘" ë¡œë“œí•˜ê²Œ í•¨ìœ¼ë¡œì¨ í”„ë ˆì„ì›Œí¬ ì´ˆê¸°í™” ì‹œ ë°œìƒí•˜ëŠ” ë¦¬ì…‹ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
        tmp_ckpt_path = None
        if args.model_path and os.path.exists(args.model_path):
            tmp_ckpt_path = convert_to_lightning_checkpoint(args.model_path, model, OUTPUT_DIR, transform=transform)
            logger.info(f"[*] ì„ì‹œ ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {tmp_ckpt_path}")

        # ================== 5. ì—”ì§„ ì„¤ì • ë° ì‹¤í–‰ ==================== #
        early_stop = EarlyStopping(monitor="image_AUROC", patience=5, mode="max", verbose=True)
        mlflow_logger = AnomalibMLFlowLogger(experiment_name="Battery_S1_AnomalyDetection", save_dir=str(OUTPUT_DIR))

        engine = Engine(
            max_epochs=args.epochs,
            accelerator="auto",
            devices=1,
            default_root_dir=str(OUTPUT_DIR),
            logger=mlflow_logger,
            callbacks=[early_stop],
            gradient_clip_val=1.0
        )

        if args.mode == "training":
            logger.info("[*] í•™ìŠµ ëª¨ë“œ ì‹œì‘")
            engine.fit(model=model, datamodule=datamodule)
        else:
            logger.info("[*] í‰ê°€ ëª¨ë“œ ì‹œì‘ (ê°€ì¤‘ì¹˜ ì£¼ì…)")
            engine.test(model=model, datamodule=datamodule, ckpt_path=tmp_ckpt_path)
        
        # ì„ê³„ê°’ ê²°ê³¼ í™•ì¸
        if hasattr(model, "image_threshold"):
            thresh = model.image_threshold.value.item() if hasattr(model.image_threshold, "value") else model.image_threshold
            logger.success(f"[*] Calculated Image Threshold: {thresh:.4f}")

        # ìµœì¢… ê°€ì¤‘ì¹˜ ì €ì¥
        model_pt_path = OUTPUT_DIR / "model.pt"
        torch.save(model.state_dict(), model_pt_path)
        logger.success(f"[FINISH] ì‘ì—… ì™„ë£Œ. ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")

    except Exception as e:
        logger.error(f"[FATAL] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        raise

if __name__ == "__main__":
    main()