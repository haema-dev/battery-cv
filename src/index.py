
import os
import torch
import argparse
import mlflow
import json
import time
import cv2
from loguru import logger
from anomalib.models import Patchcore
from anomalib.data import Folder
from anomalib.engine import Engine
from pathlib import Path
from torchvision.transforms.v2 import Resize
import numpy as np

class MaskedFolder(Folder):
    """
    ë°°ê²½ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬ ë°°í„°ë¦¬ ë³¸ì²´ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ ë§Œë“œëŠ” ë°ì´í„°ëª¨ë“ˆì…ë‹ˆë‹¤.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def _apply_mask(self, image):
        # GrabCut í˜¹ì€ ë‹¨ìˆœ thresholdë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°í„°ë¦¬ ì˜ì—­ì„ ì¶”ì¶œ (ë°°ê²½ ë…¸ì´ì¦ˆ ì œê±°)
        img_np = np.array(image)
        mask = np.zeros(img_np.shape[:2], np.uint8)
        bgdModel = np.zeros((1, 65), np.float64)
        fgdModel = np.zeros((1, 65), np.float64)
        rect = (5, 5, img_np.shape[1]-10, img_np.shape[0]-10)
        
        cv2.grabCut(img_np, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)
        mask2 = np.where((mask==2)|(mask==0), 0, 1).astype('uint8')
        img_masked = img_np * mask2[:, :, np.newaxis]
        return img_masked


def main():
    # ================== 1. Input/Output ì„¤ì • ==================== #
    parser = argparse.ArgumentParser()    
    parser.add_argument("--data_path", type=str, required=True, help="Path to mounted data asset")
    parser.add_argument('--output_dir', type=str, default='./outputs')
    parser.add_argument("--epochs", type=int, default=50)

    args = parser.parse_args()
    base_path = Path(args.data_path)
    
    logger.info("==================================================")
    logger.info("ğŸš€ S1_PatchCore_Training: [Targeted Path Mode]")
    logger.info(f"ğŸ“ ë§ˆìš´íŠ¸ ë£¨íŠ¸: {base_path}")
    logger.info("==================================================")

    # ğŸ“‚ ë°ì´í„° ê²½ë¡œ íƒìƒ‰ ë¡œì§ (ì‚¬ìš©ìë‹˜ì˜ ìš°ë ¤ë¥¼ ë°˜ì˜í•˜ì—¬ ì •ë°€í™”)
    # ì›ë³¸ ë°ì´í„°ì™€ ì„ì´ì§€ ì•Šë„ë¡ 'good' í´ë”ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
    dataset_root = None
    
    # [1ìˆœìœ„] ìš°ë¦¬ê°€ ì´ì „ì— ì„±ê³µí–ˆë˜ ê²½ë¡œ íŒ¨í„´ (train/good)
    for root, dirs, files in os.walk(base_path):
        root_path = Path(root)
        parent_name = root_path.parent.name.lower()
        current_name = root_path.name.lower()
        
        # 'train' í´ë” ì•„ë˜ì˜ 'good' í´ë”ë¥¼ ì°¾ìœ¼ë©´ 256 ë¦¬ì‚¬ì´ì¦ˆ í´ë”ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŒ
        if current_name == "good" and parent_name == "train":
            img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
            if img_count > 0:
                dataset_root = root_path
                logger.info(f"ğŸ¯ [Targeted] ìµœì ì˜ í•™ìŠµ ê²½ë¡œ ë°œê²¬: {dataset_root} ({img_count}ì¥)")
                break

    # [2ìˆœìœ„] 'good'ì´ë¼ëŠ” ì´ë¦„ì´ í¬í•¨ëœ ëª¨ë“  í´ë” ì¤‘ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê³³
    if not dataset_root:
        for root, dirs, files in os.walk(base_path):
            if "good" in root.lower():
                img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if img_count > 0:
                    dataset_root = Path(root)
                    logger.info(f"ğŸ¯ [Fallback] 'good' í‚¤ì›Œë“œ í´ë” ë°œê²¬: {dataset_root} ({img_count}ì¥)")
                    break

    if not dataset_root:
        # ë””ë²„ê¹…ì„ ìœ„í•´ í˜„ì¬ êµ¬ì¡°ë¥¼ ê°„ë‹¨íˆ ì¶œë ¥
        logger.error("âŒ 'good' í˜¹ì€ 'train/good' êµ¬ì¡°ì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.info(f"í˜„ì¬ ë£¨íŠ¸({base_path})ì˜ ì§ê³„ ìì‹ë“¤: {os.listdir(base_path)}")
        raise FileNotFoundError(f"âŒ '{base_path}' ë‚´ë¶€ì— í•™ìŠµìš© (Good) ì´ë¯¸ì§€ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ================== 2. MLflow & Output ì„¤ì • ==================== #
    mlflow.start_run()
    OUTPUT_DIR = Path(args.output_dir)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")

    try:
        # [Update] 'good' ì™¸ì— 'bad' í´ë”ë„ í¬í•¨í•˜ì—¬ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤ (AUROC ì¸¡ì •ìš©)
        test_root = dataset_root.parent / "test"
        if not test_root.exists():
             test_root = dataset_root.parent # test í´ë”ê°€ ì—†ëŠ” ê²½ìš° ë¶€ëª¨ í´ë”ì—ì„œ bad ê²€ìƒ‰
             
        # [Masked PatchCore] ë°°ê²½ì„ ì œê±°í•œ ì´ë¯¸ì§€ë§Œ í•™ìŠµí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
        datamodule = MaskedFolder(
            name="battery_masked",
            root=str(dataset_root.parent),
            normal_dir="train/good",
            test_split_mode="from_dir",
            test_dir="test",
            train_batch_size=16,
            eval_batch_size=1,
            num_workers=4,
            augmentations=Resize((512, 512)),
        )

        model = Patchcore(
            backbone="resnet18",
            layers=["layer1", "layer2", "layer3"],
            coreset_sampling_ratio=0.1,
        )
        # metricsì— AUROC, F1Score ë“±ì„ ì¶”ê°€í•˜ì—¬ ì–‘ë¶ˆ ë¶„ë¥˜ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
        engine = Engine(
            max_epochs=args.epochs, 
            accelerator="auto", 
            devices=1, 
            default_root_dir=str(OUTPUT_DIR),
            task="segmentation" # ê²°í•¨ ìœ„ì¹˜ê¹Œì§€ í™•ì¸
        )

        # ================== 4. ëª¨ë¸ í•™ìŠµ ==================== #
        logger.info(f"ğŸ§¬ S1 PatchCore í•™ìŠµ ì‹œì‘ (Target Epochs: {args.epochs})...")
        engine.fit(model=model, datamodule=datamodule)
        
        # [NEW] í‰ê°€ ìˆ˜í–‰: ì •ìƒ/ë¶ˆëŸ‰ ë¶„ë¥˜ ì„±ëŠ¥(AUROC) ë° íˆíŠ¸ë§µ ìƒì„±
        logger.info("ğŸ“Š ì„±ëŠ¥ í‰ê°€ ë° íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        engine.test(model=model, datamodule=datamodule)
        
        logger.success(f"âœ… {args.epochs} ì—í­ í•™ìŠµ ë° í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ëë‚¬ìŠµë‹ˆë‹¤!")

        # ================== 5. ê²°ê³¼ ì €ì¥ ==================== #
        torch.save(model.state_dict(), OUTPUT_DIR / "model.pt")
        info = {
            "model": "patchcore",
            "backbone": "resnet18",
            "layers": ["layer1", "layer2", "layer3"],
            "resolution": 512,
            "coreset_ratio": 0.1,
            "dataset_path": str(dataset_root),
            "epochs": args.epochs,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "task": "anomaly_detection_with_eval"
        }
        with open(OUTPUT_DIR / "info.json", 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)

        mlflow.log_params(info)
        mlflow.log_artifact(str(OUTPUT_DIR))
        logger.success("ğŸ‰ ëª¨ë“  ì‚°ì¶œë¬¼ì´ Azure MLì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()