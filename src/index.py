import argparse
import os
import sys
import json
import time
from pathlib import Path
import torch
import cv2
import numpy as np
from loguru import logger
from collections import defaultdict

# Anomalib TorchInferencer ì‚¬ìš©
try:
    from anomalib.deploy import TorchInferencer
    INFERENCER_AVAILABLE = True
except ImportError:
    INFERENCER_AVAILABLE = False

def find_validation_root(base_path):
    """ì‚¬ìš©ìë‹˜ì´ ì§€ì •í•˜ì‹  'datasets/256x256 fit/validation' ê²½ë¡œë¥¼ ì •ë°€ íƒìƒ‰í•©ë‹ˆë‹¤."""
    base = Path(base_path).resolve()
    logger.info(f"ğŸ” ê²€ì¦ ë°ì´í„° íƒìƒ‰ ì‹œì‘: {base}")
    
    # 1ìˆœìœ„: 'datasets/256x256 fit/validation' ì •ë°€ íƒìƒ‰
    for p in base.rglob("*/validation"):
        if "256x256 fit" in str(p):
            logger.success(f"âœ… ê²€ì¦ ë°ì´í„°ì…‹ ë°œê²¬: {p}")
            return p
            
    # 2ìˆœìœ„: 'validation' í´ë” íƒìƒ‰
    for p in base.rglob("validation"):
        if p.is_dir():
            logger.warning(f"âš ï¸ 'validation' í´ë” ë°œê²¬: {p}")
            return p
            
    logger.error("âŒ 'validation' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return None

def run_evaluation(data_path, model_path, output_dir):
    logger.info("==================================================")
    logger.info("ğŸš€ STAGE 2: INFERENCE & PERFORMANCE EVALUATION")
    logger.info("==================================================")

    if not INFERENCER_AVAILABLE:
        logger.error("âŒ 'TorchInferencer'ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 1. Inferencer ì´ˆê¸°í™”
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")
    
    try:
        # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸
        if not os.path.exists(model_path):
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            return
            
        # [SURGEON PATCH] 'model' key missing ëŒ€ì‘
        # TorchInferencerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ckpt['model']ì„ ì°¾ìœ¼ë ¤ í•˜ì§€ë§Œ,
        # ê°„í˜¹ ckpt ìì²´ê°€ state_dictì´ê±°ë‚˜ ë‹¤ë¥¸ êµ¬ì¡°ì¼ ê²½ìš° ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤.
        ckpt = torch.load(model_path, map_location="cpu")
        if isinstance(ckpt, dict) and "model" not in ckpt:
            logger.warning("âš ï¸ 'model' keyê°€ ì—†ìŠµë‹ˆë‹¤. ìë™ êµ¬ì¡° ë³µì› ì‹œë„...")
            # ë§Œì•½ ckpt ìì²´ê°€ state_dictì¸ ê²½ìš° 'model' keyë¡œ ê°ì‹¸ì„œ ì„ì‹œ íŒŒì¼ ìƒì„±
            temp_model_path = "/tmp/fixed_model.pt"
            os.makedirs("/tmp", exist_ok=True)
            torch.save({"model": ckpt}, temp_model_path)
            model_path = temp_model_path
            logger.success(f"âœ… êµ¬ì¡° ë³µì› ì™„ë£Œ: {model_path}")

        inferencer = TorchInferencer(path=model_path, device=device)
        logger.success("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return

    # 2. ê²½ë¡œ ì„¤ì •
    validation_root = find_validation_root(data_path)
    if not validation_root: return
    
    output_base = Path(output_dir)
    output_base.mkdir(parents=True, exist_ok=True)

    # 3. í‰ê°€ ë°ì´í„° ì´ˆê¸°í™” (Confusion Matrixìš©)
    results_summary = []
    matrix = defaultdict(int) 

    # 4. ì¹´í…Œê³ ë¦¬ ìˆœíšŒ
    categories = [d for d in validation_root.iterdir() if d.is_dir()]
    logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ëª©ë¡: {[c.name for c in categories]}")

    for cat_dir in categories:
        cat_name = cat_dir.name
        is_actual_anomaly = 0 if cat_name.lower() == "good" else 1
        
        cat_output = output_base / "heatmaps" / cat_name
        cat_output.mkdir(parents=True, exist_ok=True)
        
        img_files = list(cat_dir.glob("*.jpg")) + list(cat_dir.glob("*.png")) + list(cat_dir.glob("*.jpeg"))
        logger.info(f"ğŸ” {cat_name} ì²˜ë¦¬ ì¤‘... ({len(img_files)}ì¥)")

        for img_path in img_files:
            try:
                # ì¶”ë¡  ìˆ˜í–‰
                prediction = inferencer.predict(image=str(img_path))
                
                # ì‹œê°í™” ì €ì¥ (Heatmap)
                heatmap = prediction.heatmap
                cv2.imwrite(str(cat_output / f"heatmap_{img_path.name}"), heatmap)
                
                # ë¶„ë¥˜ ê²°ê³¼ ì¶”ì¶œ
                pred_label = int(prediction.pred_label) if hasattr(prediction, 'pred_label') else (1 if prediction.pred_score > 0.5 else 0)
                pred_score = float(prediction.pred_score)

                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if is_actual_anomaly == 0: 
                    if pred_label == 0: matrix["TN"] += 1
                    else: matrix["FP"] += 1
                else: 
                    if pred_label == 1: matrix["TP"] += 1
                    else: matrix["FN"] += 1
                
                results_summary.append({
                    "image": img_path.name,
                    "actual": "Anomaly" if is_actual_anomaly else "Normal",
                    "predicted": "Anomaly" if pred_label else "Normal",
                    "score": pred_score
                })

            except Exception as e:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨ ({img_path.name}): {e}")

    # 5. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
    total = sum(matrix.values())
    accuracy = (matrix["TP"] + matrix["TN"]) / total if total > 0 else 0
    
    logger.info("--------------------------------------------------")
    logger.info("ğŸ“Š STAGE 2 EVALUATION REPORT")
    logger.info(f"âœ… Accuracy: {accuracy:.4f}")
    logger.info(f"ğŸ“ Confusion Matrix: {dict(matrix)}")
    logger.info("--------------------------------------------------")

    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    report = {
        "metrics": dict(matrix),
        "overall_accuracy": accuracy,
        "details": results_summary
    }
    with open(output_base / "evaluation_report.json", "w") as f:
        json.dump(report, f, indent=4)
    
    logger.success(f"ğŸ‰ Stage 2 ì™„ë£Œ. íˆíŠ¸ë§µ ë° ë¦¬í¬íŠ¸ ì €ì¥ë¨: {output_dir}")

if __name__ == "__main__":
    # ë””ë²„ê¹…: ì—ì €ì—ì„œ ë“¤ì–´ì˜¤ëŠ” ì›ë³¸ ì¸ì í™•ì¸
    logger.info(f"ğŸ“‹ Raw Arguments: {sys.argv}")
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="Path to input validation folders")
    parser.add_argument("--model_path", type=str, required=True, help="Path to trained model.pt")
    parser.add_argument("--output_dir", type=str, required=True, help="Folder to save results")
    
    try:
        args = parser.parse_args()
        logger.info(f"âœ… Parsed Arguments: data={args.data_path}, model={args.model_path}, out={args.output_dir}")
        
        sys.stdout.reconfigure(line_buffering=True)
        run_evaluation(args.data_path, args.model_path, args.output_dir)
    except Exception as e:
        logger.error(f"âŒ FATAL: Argument issue: {e}")
        sys.exit(1)