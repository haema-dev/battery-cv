import argparse
import os
import sys
import inspect
from pathlib import Path
import torch
import lightning.pytorch.trainer.trainer as trainer_module

# v3.6: "The Master Surgeon" - Ultimate Argument Filtering Fix
# Anomalib 1.1.3's Engine leaks internal arguments (task, pixel_metrics, etc.) 
# into the PyTorch Lightning Trainer, causing continuous TypeErrors.
# This patch dynamically filters out any arguments that the Trainer doesn't recognize.

original_trainer_init = trainer_module.Trainer.__init__
# Trainerê°€ ì‹¤ì œë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” ì¸ì ëª©ë¡ì„ ë¯¸ë¦¬ íŒŒì•…í•©ë‹ˆë‹¤.
TRAINER_ALLOWED_PARAMS = set(inspect.signature(original_trainer_init).parameters.keys())

def patched_trainer_init(self, *args, **kwargs):
    # Trainerê°€ ëª¨ë¥´ëŠ” ì¸ìë“¤ì€ ëª¨ë‘ ê°€ì§€ì¹˜ê¸°(Filter) í•©ë‹ˆë‹¤.
    filtered_kwargs = {k: v for k, v in kwargs.items() if k in TRAINER_ALLOWED_PARAMS}
    
    removed_params = set(kwargs.keys()) - TRAINER_ALLOWED_PARAMS
    if removed_params:
        print(f"ğŸ©¹ [Master Surgeon] Filtered out invalid Trainer arguments: {removed_params}")
        
    return original_trainer_init(self, *args, **filtered_kwargs)

trainer_module.Trainer.__init__ = patched_trainer_init

from anomalib.data import Folder
from anomalib.models import Fastflow
from anomalib.engine import Engine

def find_anomalib_root(base_path):
    base = Path(base_path)
    for p in base.rglob("*"):
        if p.is_dir() and p.name.lower() == "train":
            return p.parent
    return base

def run_pipeline(data_path, output_dir, epochs):
    print("==================================================")
    print("ğŸš€ STAGE 1: DEFINITIVE STABILIZATION V3.6 (MASTER)")
    print("==================================================")
    
    # 1. ë°ì´í„° ë£¨íŠ¸ íƒìƒ‰
    optimized_root = find_anomalib_root(data_path)
    print(f"ğŸ” Final Data Root: {optimized_root}")

    # 2. Folder ë™ì  ì¸ì ì„¤ì • (ê²€ì¦ ì™„ë£Œëœ ë¡œì§)
    sig_folder = inspect.signature(Folder)
    dm_args = {
        "name": "battery",
        "root": str(optimized_root),
        "normal_dir": "train/good",
        "test_split_mode": "from_dir"
    }
    if "normal_test_dir" in sig_folder.parameters: 
        dm_args["normal_test_dir"] = "test/normal"
    
    for k in ["abnormal_dir", "abnormal_test_dir", "test_abnormal_dir"]:
        if k in sig_folder.parameters:
            dm_args[k] = "test/damaged"
            break
    
    print(f"ğŸ› ï¸ Built Datamodule Args: {dm_args}")
    datamodule = Folder(**dm_args)

    # 3. ëª¨ë¸ ì„¤ì •
    model = Fastflow(backbone="resnet18", flow_steps=8)

    # 4. ì—”ì§„ ì„¤ì •
    # ì´ì œ 'Master Surgeon'ì´ ëª¨ë¥´ëŠ” ì¸ìëŠ” ìë™ìœ¼ë¡œ ê¹ì•„ë‚´ë¯€ë¡œ 
    # ë§ˆìŒ í¸íˆ í•„ìš”í•œ ì¸ìë“¤ì„ ì „ë‹¬í•©ë‹ˆë‹¤.
    print("âš™ï¸ Initializing Engine with Classification task...")
    engine = Engine(
        max_epochs=epochs,
        default_root_dir=output_dir,
        devices=1,
        accelerator="auto",
        task="classification",
        pixel_metrics=None # gt_mask ì—ëŸ¬ ë°©ì§€ìš©
    )

    # 5. ì‹¤í–‰
    print(f"\nâ³ Starting Engine.fit (Epochs: {epochs})...")
    try:
        engine.fit(model=model, datamodule=datamodule)
    except Exception as e:
        print(f"\nâŒ FINAL FAILURE: {e}")
        raise e

    # 6. ì €ì¥
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    model_save_path = output_path / "model.pt"
    torch.save(model.state_dict(), model_save_path)
    print(f"\nâœ… SUCCESS: Stage 1 Training Complete.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--epochs", type=int, default=100)
    args = parser.parse_args()
    sys.stdout.reconfigure(line_buffering=True)
    run_pipeline(args.data_path, args.output_dir, args.epochs)