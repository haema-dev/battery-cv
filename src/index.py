
import os
import torch
import argparse
import mlflow
import json
import time
from loguru import logger
from anomalib.models import Fastflow
from anomalib.data import Folder
from anomalib.engine import Engine
from pathlib import Path
from torchvision.transforms.v2 import Resize

def list_all_contents(path, max_files=10):
    """ë””ë²„ê¹…ì„ ìœ„í•´ ë§ˆìš´íŠ¸ëœ ëª¨ë“  êµ¬ì¡°ë¥¼ ìƒ…ìƒ…ì´ ë¡œê¹…í•©ë‹ˆë‹¤."""
    try:
        path = Path(path)
        logger.info(f"ï¿½ [DEEP DEBUG] Full structure of {path}:")
        for root, dirs, files in os.walk(path):
            level = len(Path(root).relative_to(path).parts)
            indent = "  " * level
            logger.info(f"{indent}ğŸ“ {os.path.basename(root) or '/'} ({len(files)} files)")
            # íŒŒì¼ ì¼ë¶€ ì¶œë ¥
            for f in files[:max_files]:
                logger.info(f"{indent}  - ğŸ“„ {f}")
            if len(files) > max_files:
                logger.info(f"{indent}  - ... and {len(files)-max_files} more files")
    except Exception as e:
        logger.error(f"âŒ Deep listing failed: {e}")

def main():
    # ================== 1. Input/Output ì„¤ì • ==================== #
    parser = argparse.ArgumentParser()    
    parser.add_argument("--data_path", type=str, required=True, help="Path to mounted data asset")
    parser.add_argument('--output_dir', type=str, default='./outputs')
    parser.add_argument("--epochs", type=int, default=1)

    args = parser.parse_args()
    base_path = Path(args.data_path)
    
    logger.info("==================================================")
    logger.info("ğŸš€ S1_FastFlow_Training: [Hyper-Robust Mode]")
    logger.info(f"ğŸ“ ë§ˆìš´íŠ¸ ë£¨íŠ¸: {base_path}")
    logger.info("==================================================")

    # 1. ì¼ë‹¨ ë‹¤ ì°ì–´ë³´ê¸° (ì›ì¸ íŒŒì•…ìš©)
    list_all_contents(base_path)

    # ğŸ“‚ ë°ì´í„° ê²½ë¡œ íƒìƒ‰ ë¡œì§ (ìš°ì„ ìˆœìœ„)
    dataset_root = None
    
    # [ìš°ì„ ìˆœìœ„ 1] ëª…ì‹œì  ê²½ë¡œ
    explicit_path = base_path / "datasets" / "resized" / "train" / "good"
    if explicit_path.exists() and any(explicit_path.iterdir()):
        dataset_root = explicit_path
        logger.info(f"âœ… [Priority 1] ëª…ì‹œì  ê²½ë¡œ ë°œê²¬: {dataset_root}")

    # [ìš°ì„ ìˆœìœ„ 2] í‚¤ì›Œë“œ ì¡°í•© ê²€ìƒ‰
    if not dataset_root:
        for root, dirs, files in os.walk(base_path):
            root_path = Path(root)
            parts = [p.lower() for p in root_path.parts]
            if "resized" in parts and "good" in parts:
                img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if img_count > 0:
                    dataset_root = root_path
                    logger.info(f"âœ… [Priority 2] í‚¤ì›Œë“œ ê²€ìƒ‰ ë°œê²¬: {dataset_root} ({img_count}ì¥)")
                    break

    # [ìš°ì„ ìˆœìœ„ 3] ì´ë¦„ì´ 'good'ì¸ í´ë” ì¤‘ ì´ë¯¸ì§€ê°€ ìˆëŠ” ê³³
    if not dataset_root:
        for root, dirs, files in os.walk(base_path):
            if os.path.basename(root).lower() == "good":
                img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if img_count > 0:
                    dataset_root = Path(root)
                    logger.info(f"âœ… [Priority 3] 'good' í´ë” ë°œê²¬: {dataset_root} ({img_count}ì¥)")
                    break

    # [ìš°ì„ ìˆœìœ„ 4] ê·¸ëƒ¥ ì´ë¯¸ì§€ê°€ ê°€ì¥ ë§ì€ í´ë” (ìµœí›„ì˜ ë³´ë£¨)
    if not dataset_root:
        max_imgs = 0
        best_path = None
        for root, dirs, files in os.walk(base_path):
            img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
            if img_count > max_imgs:
                max_imgs = img_count
                best_path = Path(root)
        if best_path and max_imgs > 0:
            dataset_root = best_path
            logger.info(f"âœ… [Priority 4] ìµœëŒ€ ì´ë¯¸ì§€ í´ë” ì„ íƒ: {dataset_root} ({max_imgs}ì¥)")

    if not dataset_root:
        # ì •ë§ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´ ë£¨íŠ¸ ìì²´ë¼ë„ ì‹œë„ (íŒŒì¼ì´ ë£¨íŠ¸ì— ìˆì„ ìˆ˜ë„ ìˆìŒ)
        img_count = len([f for f in os.listdir(base_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
        if img_count > 0:
            dataset_root = base_path
            logger.info(f"âœ… [Priority 5] ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„ íƒ: {dataset_root} ({img_count}ì¥)")

    if not dataset_root:
        raise FileNotFoundError(f"âŒ '{base_path}' ë‚´ë¶€ì—ì„œ ì–´ë–¤ ì´ë¯¸ì§€ í˜•ì‹ë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìì‚° êµ¬ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # ================== 2. MLflow & Output ì„¤ì • ==================== #
    mlflow.start_run()
    OUTPUT_DIR = Path(args.output_dir)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")

    try:
        # ================== 3. Anomalib ë°ì´í„° êµ¬ì„± ==================== #
        logger.info(f"ğŸ“¥ ë°ì´í„°ì…‹ ë¡œë“œ: {dataset_root}")
        transform = Resize((256, 256))
        
        datamodule = Folder(
            name="battery_resized",
            root=str(dataset_root),
            normal_dir=".",
            train_batch_size=32,
            eval_batch_size=8,
            num_workers=4,
            augmentations=transform,
        )

        model = Fastflow(backbone="resnet18", flow_steps=8, evaluator=False)
        engine = Engine(max_epochs=args.epochs, accelerator="auto", devices=1, default_root_dir=str(OUTPUT_DIR))

        # ================== 4. ëª¨ë¸ í•™ìŠµ ==================== #
        logger.info(f"ğŸ§¬ ëª¨ë¸ í•™ìŠµ ì§„í–‰ (Epochs: {args.epochs})...")
        engine.fit(model=model, datamodule=datamodule)
        logger.success("âœ… í•™ìŠµ ì™„ë£Œ!")

        # ================== 5. ê²°ê³¼ ì €ì¥ ==================== #
        torch.save(model.state_dict(), OUTPUT_DIR / "model.pt")
        info = {"dataset": str(dataset_root), "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")}
        with open(OUTPUT_DIR / "info.json", 'w') as f: json.dump(info, f, indent=2)

        mlflow.log_params(info)
        mlflow.log_artifact(str(OUTPUT_DIR))
        logger.success("ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()