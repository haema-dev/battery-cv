
import os
import torch
import argparse
import mlflow
import json
import time
from loguru import logger
from anomalib.models import Fastflow
from anomalib.data import Folder
from anomalib.engine import Engine
from pathlib import Path
from torchvision.transforms.v2 import Resize

def list_directory_contents(path, depth=2):
    """ë””ë²„ê¹…ì„ ìœ„í•´ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë¡œê¹…í•©ë‹ˆë‹¤."""
    try:
        path = Path(path)
        logger.info(f"ğŸ“‚ [DEBUG] Listing {path}:")
        for root, dirs, files in os.walk(path):
            level = len(Path(root).relative_to(path).parts)
            if level < depth:
                indent = "  " * level
                logger.info(f"{indent}ğŸ“ {os.path.basename(root)}/ ({len(files)} files)")
    except Exception as e:
        logger.error(f"âŒ Directory listing failed: {e}")

def main():
    # ================== 1. Input/Output ì„¤ì • ==================== #
    parser = argparse.ArgumentParser()    
    parser.add_argument("--data_path", type=str, required=True, help="Path to mounted data asset")
    parser.add_argument('--output_dir', type=str, default='./outputs')
    parser.add_argument("--epochs", type=int, default=1)

    args = parser.parse_args()
    
    # Path resolve() ëŒ€ì‹  ì§ì ‘ ì‚¬ìš© (ë§ˆìš´íŠ¸ ì§€ì ì—ì„œ ê°€ë” ì´ìŠˆ ë°œìƒ ë°©ì§€)
    base_path = Path(args.data_path)
    
    logger.info("==================================================")
    logger.info("ğŸš€ S1_FastFlow_Training: [Robust Path Search Mode]")
    logger.info(f"ğŸ“ ë§ˆìš´íŠ¸ ë£¨íŠ¸: {base_path}")
    logger.info("==================================================")

    # ë””ë²„ê¹…: ë£¨ë“œ ë””ë ‰í† ë¦¬ ë‚´ìš© ì¶œë ¥
    list_directory_contents(base_path, depth=3)

    # ğŸ“‚ ë°ì´í„° ê²½ë¡œ ìë™ íƒìƒ‰ (ê°€ì¥ ì •í™•í•œ 'good' í´ë” ì°¾ê¸°)
    dataset_root = None
    
    # 1. ëª…ì‹œì  ê²½ë¡œ í™•ì¸
    explicit_path = base_path / "datasets" / "resized" / "train" / "good"
    if explicit_path.exists():
        dataset_root = explicit_path
        logger.info(f"âœ… ëª…ì‹œì  ê²½ë¡œ ë°œê²¬: {dataset_root}")
    else:
        # 2. os.walkë¥¼ ì´ìš©í•œ ìœ ì—°í•œ ê²€ìƒ‰ (resized + train + good í‚¤ì›Œë“œ ì¡°í•©)
        logger.warning("âš ï¸ ëª…ì‹œì  ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•´ ì „ì²´ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        for root, dirs, files in os.walk(base_path):
            root_path = Path(root)
            parts = [p.lower() for p in root_path.parts]
            
            # 'resized', 'train', 'good'ì´ ëª¨ë‘ ê²½ë¡œì— í¬í•¨ë˜ë©´ì„œ ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” íƒìƒ‰
            if "resized" in parts and "train" in parts and "good" in parts:
                img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if img_count > 0:
                    dataset_root = root_path
                    logger.info(f"ğŸ¯ ìœ ì—°í•œ ê²€ìƒ‰ìœ¼ë¡œ ê²½ë¡œ ë°œê²¬: {dataset_root} (ì´ë¯¸ì§€ {img_count}ì¥)")
                    break

    if not dataset_root:
        # 3. ìµœí›„ì˜ ìˆ˜ë‹¨: 'good' í´ë” ì¤‘ ì´ë¯¸ì§€ê°€ 100ì¥ ì´ìƒì¸ ê³³ íƒìƒ‰
        for root, dirs, files in os.walk(base_path):
            if os.path.basename(root).lower() == "good":
                img_count = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
                if img_count > 100:
                    dataset_root = Path(root)
                    logger.info(f"ğŸ”„ ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ ê²½ë¡œ ë°œê²¬: {dataset_root} ({img_count}ì¥)")
                    break

    if not dataset_root:
        raise FileNotFoundError(f"âŒ '{base_path}' ë‚´ë¶€ì—ì„œ í•™ìŠµìš© 'good' ì´ë¯¸ì§€ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ================== 2. MLflow & Output ì„¤ì • ==================== #
    mlflow.start_run()
    OUTPUT_DIR = Path(args.output_dir)
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ì¥ì¹˜: {device}")

    try:
        # ================== 3. Anomalib ë°ì´í„° êµ¬ì„± ==================== #
        logger.info("ğŸ“¥ Anomalib ë°ì´í„° ëª¨ë“ˆ êµ¬ì„± ì¤‘...")
        transform = Resize((256, 256))
        
        datamodule = Folder(
            name="battery_resized",
            root=str(dataset_root),
            normal_dir=".",
            train_batch_size=32,
            eval_batch_size=8,
            num_workers=4,
            augmentations=transform,
        )

        # ëª¨ë¸ ì´ˆê¸°í™” (FastFlow)
        model = Fastflow(
            backbone="resnet18",
            flow_steps=8,
            evaluator=False 
        )

        # Engine ì„¤ì •
        engine = Engine(
            max_epochs=args.epochs,
            accelerator="auto",
            devices=1,
            default_root_dir=str(OUTPUT_DIR),
            enable_checkpointing=True,
        )

        # ================== 4. ëª¨ë¸ í•™ìŠµ ==================== #
        logger.info(f"ğŸ§¬ ëª¨ë¸ í•™ìŠµ ì§„í–‰ (Epochs: {args.epochs})...")
        engine.fit(model=model, datamodule=datamodule)
        logger.success("âœ… í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")

        # ================== 5. ëª¨ë¸ ê°€ì§ì¹˜ ì €ì¥ ==================== #
        model_save_path = OUTPUT_DIR / "model.pt"
        torch.save(model.state_dict(), model_save_path)
        logger.info(f"ğŸ’¾ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ: {model_save_path}")

        # ì‘ì—… ì •ë³´ ê¸°ë¡
        info = {
            "experiment": "Battery_S1_AnomalyDetection",
            "mode": "training_only",
            "dataset_path": str(dataset_root),
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        with open(OUTPUT_DIR / "info.json", 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2, ensure_ascii=False)

        mlflow.log_params(info)
        mlflow.log_artifact(str(OUTPUT_DIR))
        logger.success("ğŸ‰ Azure ML ê²°ê³¼ ì €ì¥ ë° ì‹¤í—˜ ì¢…ë£Œ!")

    except Exception as e:
        logger.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        mlflow.end_run()

if __name__ == "__main__":
    main()