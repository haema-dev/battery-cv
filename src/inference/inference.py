import os
import argparse
from pathlib import Path
import torch
import cv2
import numpy as np

from anomalib.engine import Engine
from anomalib.models import Fastflow

# [Security Fix] PyTorch 2.4+ requires explicit trust for custom code
os.environ["TRUST_REMOTE_CODE"] = "1"

# [Backward Compatibility] Old models pickled with TunableFastflow
class TunableFastflow(Fastflow):
    def __init__(self, *args, **kwargs):
        kwargs.pop('lr', None)
        kwargs.pop('weight_decay', None)
        super().__init__(*args, **kwargs)

try:
    from preprocess import preprocess_image
except ImportError:
    print("âš ï¸ Warning: 'src/preprocess.py' not found. Ensure it exists.")
    preprocess_image = None

def run_inference(data_path, model_path, output_dir, skip_preprocess=False):
    """
    Runs inference using Anomalib Engine.predict() API.
    """
    print("--------------------------------------------------")
    print(f"ğŸš€ [Phase 2] Inference & Heatmap Generation")
    print(f"ğŸ“¦ Model Path: {model_path}")
    print(f"ğŸ“‚ Data Path: {data_path}")
    print(f"âš™ï¸ Skip Preprocess: {skip_preprocess}")
    print("--------------------------------------------------")

    # 1. Device and Model Resolution
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ Using device: {device}")

    model_path_obj = Path(model_path)
    if model_path_obj.is_dir():
        # Priority: model.pt (exported) -> model.ckpt (checkpoint)
        all_pt = list(model_path_obj.rglob("model.pt"))
        all_ckpt = list(model_path_obj.rglob("*.ckpt"))
        candidates = all_pt + all_ckpt
        if not candidates:
            print(f"âŒ Error: No valid model file found in {model_path}")
            return
        actual_model_file = candidates[0]
        print(f"ğŸ“ Selected model for inference: {actual_model_file}")
    else:
        actual_model_file = model_path_obj

    # 2. Load model
    print(f"ğŸ“¥ Loading model from: {actual_model_file}")

    # Load based on file type
    if str(actual_model_file).endswith('.ckpt'):
        # Lightning checkpoint
        model = Fastflow.load_from_checkpoint(str(actual_model_file))
    else:
        # Torch export (.pt)
        checkpoint = torch.load(str(actual_model_file), map_location=device, weights_only=False)
        if isinstance(checkpoint, dict) and "model" in checkpoint:
            model = checkpoint["model"]
        else:
            print(f"âŒ Unexpected checkpoint format")
            return

    model.eval()
    print(f"âœ… Model loaded successfully")

    # 3. Setup Engine
    engine = Engine(
        accelerator="auto",
        devices=1,
        default_root_dir=str(output_dir)
    )

    data_base = Path(data_path)
    output_base = Path(output_dir)

    # Category discovery
    categories = [d for d in data_base.iterdir() if d.is_dir() and not d.name.startswith('.')]
    if (data_base / "test").exists():
        data_base = data_base / "test"
        categories = [d for d in data_base.iterdir() if d.is_dir() and not d.name.startswith('.')]

    if not categories:
        print(f"âŒ Error: No category folders found in {data_base}")
        return

    print(f"ğŸ¯ Found categories: {[c.name for c in categories]}")

    # 4. Process each category
    for cat_dir in categories:
        cat_name = cat_dir.name
        cat_output = output_base / cat_name
        cat_output.mkdir(parents=True, exist_ok=True)

        print(f"ğŸ” Processing: {cat_name}")

        # Engine.predict with data_path
        try:
            predictions = engine.predict(
                model=model,
                data_path=str(cat_dir),
                return_predictions=True
            )
        except Exception as e:
            print(f"âš ï¸ Prediction failed for {cat_name}: {e}")
            continue

        # Extract results from list of batches
        for batch in predictions:
            # batch is ImageBatch
            paths = batch.item_path  # List of paths
            anomaly_maps = batch.anomaly_map  # (B, H, W) or (B, 1, H, W)
            pred_masks = batch.pred_mask if hasattr(batch, 'pred_mask') else None

            batch_size = len(paths)
            for i in range(batch_size):
                img_path = Path(paths[i])

                # Original image for overlay (read from disk)
                orig_img = cv2.imread(str(img_path))
                if orig_img is None:
                    continue

                # Anomaly map processing
                if anomaly_maps is not None:
                    amap = anomaly_maps[i].detach().cpu().numpy()
                    if amap.ndim == 3:  # (1, H, W)
                        amap = amap.squeeze(0)

                    # Normalize to 0-255
                    min_val, max_val = amap.min(), amap.max()
                    if max_val - min_val > 0:
                        amap_norm = ((amap - min_val) / (max_val - min_val) * 255).astype(np.uint8)
                    else:
                        amap_norm = np.zeros_like(amap, dtype=np.uint8)

                    # Resize to match original image if needed
                    if amap_norm.shape != orig_img.shape[:2]:
                        amap_norm = cv2.resize(amap_norm, (orig_img.shape[1], orig_img.shape[0]))

                    # Apply colormap and overlay
                    heatmap = cv2.applyColorMap(amap_norm, cv2.COLORMAP_JET)
                    overlay = cv2.addWeighted(orig_img, 0.6, heatmap, 0.4, 0)
                    cv2.imwrite(str(cat_output / f"heatmap_{img_path.name}"), overlay)

                # Pred mask processing
                if pred_masks is not None:
                    mask = pred_masks[i].detach().cpu().numpy()
                    if mask.ndim == 3:
                        mask = mask.squeeze(0)

                    if mask.max() > 0:
                        if mask.shape != orig_img.shape[:2]:
                            mask = cv2.resize(mask, (orig_img.shape[1], orig_img.shape[0]), interpolation=cv2.INTER_NEAREST)

                        mask_overlay = orig_img.copy()
                        mask_overlay[mask > 0.5] = [0, 0, 255]  # Red
                        overlay_mask = cv2.addWeighted(orig_img, 0.7, mask_overlay, 0.3, 0)
                        cv2.imwrite(str(cat_output / f"mask_{img_path.name}"), overlay_mask)

    print(f"\nâœ… All heatmaps generated successfully!")
    print(f"ğŸ“ Location: {output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_path", type=str, required=True, help="Input data folder")
    parser.add_argument("--model_path", type=str, required=True, help="Trained model (.pt) file")
    parser.add_argument("--output_dir", type=str, required=True, help="Inference results folder")
    parser.add_argument("--skip_preprocess", action="store_true", help="Skip custom preprocessing (use for pipeline)")

    args = parser.parse_args()
    run_inference(args.data_path, args.model_path, args.output_dir, args.skip_preprocess)
